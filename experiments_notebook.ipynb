{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s15t283/svm_test_d/env/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from fastFM import als\n",
    "from scipy.sparse import csr_matrix\n",
    "import MeCab\n",
    "import numpy as np\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import csv\n",
    "from gensim import matutils\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import linear_model\n",
    "import codecs\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import classification_report\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分散表現の読み込み\n",
    "- めっちゃ時間かかる\n",
    "- これは一度実行した後は，なんども実行しなくてOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_w2v_hottolink = \"w2v_all_vector200_win5_sgns0.vec\"\n",
    "model_hottolink = KeyedVectors.load_word2vec_format(file_w2v_hottolink, binary=False)\n",
    "#model_hottolink = word2vec.Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章をmecabで分かちがきして、名詞・動詞・形容詞の単語一覧を返す\n",
    "def wakati(text):\n",
    "    tagger = MeCab.Tagger()\n",
    "    tagger.parse('')\n",
    "    node = tagger.parseToNode(text)\n",
    "    word_list = []\n",
    "    while node:\n",
    "        features = node.feature.split(\",\")\n",
    "        pos = features[0]\n",
    "#        if pos in [\"名詞\", \"動詞\", \"形容詞\", \"助動詞\"]:\n",
    "        lemma = node.surface\n",
    "        word_list.append(lemma)\n",
    "            # 標準形に変更\n",
    "        node = node.next\n",
    "\n",
    "    return word_list[1:-1]\n",
    "\n",
    "def parse():\n",
    "    lines = []\n",
    "    #for line in open('lastdata1_text.txt', 'r'):\n",
    "    for line in open('./sample_LDA_setdata/all_smptons_set1000_text.txt', 'r'):\n",
    "        arr = line.split(\"\\t\")\n",
    "        lines.append(arr)\n",
    "    return lines\n",
    "\n",
    "def vec2dense(vec, num_terms):\n",
    "    return list(matutils.corpus2dense([vec], num_terms=num_terms).T[0])\n",
    "#    return np.array(list(matutils.corpus2dense([vec], num_terms=num_terms).T[0]))\n",
    "def scores2mean(scores: List[float]) -> float:\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分散表現の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "50\n",
      "1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s15t283/svm_test_d/env/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nf2 = open('./fit_data/text_word_count.csv', 'r')\\ndataReader = csv.reader(f2)\\nfor i , row in enumerate(dataReader):\\n    vectors[i] = np.append(vectors[i],float(row[0]))\\n\\nf = open('./fit_data/text_body_part.csv', 'r') \\ndataReader = csv.reader(f)\\nfor i , row in enumerate(dataReader):\\n    vectors[i] = np.append(vectors[i],float(row[0]))\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wakatis = []\n",
    "wakatis1 = []\n",
    "wakatis_bow = []\n",
    "wakatis_bow1 = []\n",
    "wakatis_bow = pd.read_csv(\"./sample_LDA_setdata/all_smptons_set1000_tsutsuzi.csv\",header = None).values.tolist()\n",
    "wakatis_bow1= pd.read_csv(\"./sample_LDA_setdata/all_smptons_set1000_LDA.csv\",header = None).values.tolist()\n",
    "print(len(wakatis_bow[0]))\n",
    "print(len(wakatis_bow1[0]))\n",
    "print(len(wakatis_bow))\n",
    "vectors = []\n",
    "vectors_bow = []\n",
    "lines = [t[0] for t in parse()]\n",
    "for line in lines:\n",
    "#    wakatis_bow.append(wakati(line))\n",
    "    words = wakati(line)    \n",
    "    embeddings = np.zeros(200)\n",
    "    count = 0\n",
    "    for line1 in words:\n",
    "        if line1 in model_hottolink.wv:\n",
    "            embeddings += model_hottolink[line1]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        embeddings /= count\n",
    "    vectors.append(deepcopy(embeddings))\n",
    "\n",
    "print(len(vectors))\n",
    "    \n",
    "\n",
    "#  素性追加  ##  \n",
    "\n",
    "\"\"\"\n",
    "f2 = open('./fit_data/text_word_count.csv', 'r')\n",
    "dataReader = csv.reader(f2)\n",
    "for i , row in enumerate(dataReader):\n",
    "    vectors[i] = np.append(vectors[i],float(row[0]))\n",
    "\n",
    "f = open('./fit_data/text_body_part.csv', 'r') \n",
    "dataReader = csv.reader(f)\n",
    "for i , row in enumerate(dataReader):\n",
    "    vectors[i] = np.append(vectors[i],float(row[0]))\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 追加素性\n",
    "- 体の部位\n",
    "- 病名\n",
    "- 単語数\n",
    "\n",
    "- その他考えられそうなもの\n",
    "    - 文字数（文長）\n",
    "    - BoW, PMI, word2vecを変えながら実験してみる\n",
    "    - つつじ，ずんだ，この辺りはまた今度考える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(60 unique tokens: ['0951Q.1xx.03n01', '0751M.1xx.03n01', '0961Q.4xx.03n01', '3261M.1xx.46n01', '0961Q.2rx.03n01']...)\n",
      "[0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "250\n",
      "1701\n",
      "310\n",
      "1701\n",
      "[ 0.          0.13988405  0.          0.          0.          0.\n",
      "  0.06679823  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.24481283  0.17313408  0.          0.\n",
      "  0.08548806  0.          0.          0.          0.          0.\n",
      "  0.3184396   0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          1.38449832 -1.91864789 -1.36573228 -0.44868183\n",
      " -0.43701794  0.75139243  0.94758765 -1.05211136 -1.12704403 -1.25714665\n",
      " -0.43054632  0.57869546 -0.04318642  0.1317109   1.41145851  0.45100371\n",
      " -1.92065939 -0.20159236  0.40183058  0.33561963  2.26249808  0.0666333\n",
      "  1.21015073 -0.04312896  3.30113373 -2.84063125  2.59613114  0.10257151\n",
      " -0.36657735 -0.56741719 -1.85379276 -2.23011245  1.50815722 -0.26296148\n",
      "  1.24971269 -0.79783727 -2.86937387  1.09440591  2.00221154 -2.78085482\n",
      " -1.36160092  1.12385009 -1.31013797 -2.35822999 -1.54514988  0.89684055\n",
      "  0.16014604 -0.10001567 -0.74311611  2.04269793 -0.03773565  0.90084877\n",
      "  0.62035613 -1.2242102   2.20355499  0.37636277  0.67182417 -1.32053212\n",
      "  0.20115723  0.09001045  1.42502257 -0.02423449  0.47115319  0.69137062\n",
      "  0.81090396 -0.17843458  1.80331388  2.59512151 -5.05337467  5.23221329\n",
      " -0.16017505  0.19716287  2.10138443  1.99012987  0.18318693  0.28903627\n",
      "  2.20616615 -1.07520275 -1.09233353 -2.65525708  1.50418998  1.13568188\n",
      "  1.30031057 -0.65599711  1.4501414  -0.6387174  -0.07997283  2.76611932\n",
      "  1.3983714   0.96866247  1.80265773  0.58215795  2.48103952  2.54100973\n",
      " -2.03801064 -0.19395444  1.06833989 -1.35824582  1.15064023  0.97079068\n",
      "  0.32488364  1.80018816 -0.60978817  0.32591573 -1.48253497  0.49140225\n",
      "  0.70952933  1.61799253  0.06330314 -0.14330239  0.45271014  0.42393859\n",
      "  1.14919674 -0.50888105 -0.88480563 -1.99798231 -1.98756845 -0.14432245\n",
      "  0.25533357 -0.41283354  2.01493159  2.20398123 -1.48333906 -0.75813237\n",
      "  0.49125409 -1.99003042 -2.3053806   1.73937772 -0.42983623 -2.71908335\n",
      "  0.84830261 -1.42519616 -1.13844878 -0.35893792  2.13969866 -0.87895072\n",
      "  4.10450823  0.24054207 -0.91390593 -3.04834974 -2.44450893  0.2566461\n",
      "  1.09602479 -0.21838801  1.64400134 -1.16993006  0.07863849  0.13869831\n",
      " -0.9599676  -0.58428333 -0.08238841  3.21866582  0.91333213  2.27644215\n",
      "  1.46777664 -0.77900704  0.3380102  -0.42660233  1.40910615 -0.84065577\n",
      "  0.09841803  1.17598955 -0.91323557  2.84035791  1.14597709 -0.91458786\n",
      " -0.83462222  1.98612954 -1.98938551  2.3877723   0.39527825  1.76797198\n",
      "  0.949116    0.35616695  2.05219178 -1.13991977  1.2168034   1.27010938\n",
      "  0.4679808  -0.35038218  0.32119029  1.40533133  0.39716498 -1.38759289\n",
      "  1.71902645 -1.51389682 -0.6652458   2.7767414  -0.40463834 -0.13099055\n",
      "  1.71006465  1.25553081 -0.13150682 -0.58917013 -1.00934852 -0.54475113\n",
      " -0.93852693 -0.33817133  2.39927522 -2.26682795]\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.28076512  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.06571499  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.75837481 -0.75790426 -0.11856069  0.6852673\n",
      "  0.12878575  0.18797917  0.37777386  0.45056213 -1.11714938 -0.64406145\n",
      " -1.42037721  0.29610806 -0.38938786  0.47688608  1.40177051  1.71802141\n",
      " -1.17355848 -0.71750313  0.52622521  0.06456059  2.16301216 -0.03433926\n",
      "  0.64434215  0.62496095  2.76397342 -2.81773569  0.00879506 -0.51569461\n",
      "  1.57380655 -0.76451286 -1.2862119  -1.82493255  1.85028176 -0.42016544\n",
      "  1.71010054 -0.55686349 -1.4515058   2.19412278  3.06598345 -1.72179792\n",
      " -2.02935039  1.09734168 -0.27624011 -1.75857536 -0.07252442  1.38188328\n",
      " -0.96836656  0.8750996  -0.19311636  1.26894179  0.38221619 -0.75952737\n",
      "  0.66366873 -2.02273081  2.5112431  -0.11503933  0.926401   -0.18004568\n",
      "  0.42205468 -1.09026656  1.66562305 -0.90169561  0.93616283  0.68906402\n",
      "  1.74328783  0.48573673  1.6980873   1.87423453 -3.42562952  4.82772499\n",
      " -0.88959037  0.49062103  2.45464198  1.40489657  0.42849372 -0.18123899\n",
      "  0.91591104 -2.1563139   0.12528693 -2.29575634  0.56624324  1.58563085\n",
      "  3.74962665 -0.85014392  1.54465495  0.82414045  0.78796686  0.45168513\n",
      "  1.74474493  0.57025023  2.48365305  0.42067402  1.28158132  2.63632638\n",
      " -2.37774058 -2.8312476   0.62940387 -0.42978024  0.7366463   0.89691644\n",
      " -1.113423    2.65711748 -0.83487665  0.64129826 -1.2857902   0.61908009\n",
      " -0.22467671  3.19395422  1.28027641 -1.14530699  0.38616933  0.68643507\n",
      "  0.92483204  0.44620827  0.0719118  -1.38289306 -1.10280062  1.16793812\n",
      " -0.65620429 -1.03230534  2.44528719  2.59293611 -1.20045901 -1.49259991\n",
      " -1.02956822 -1.60597116 -2.9520094   0.97490985 -0.19010623 -0.98057769\n",
      "  0.90108752 -0.7846908  -0.55428635 -0.7061403   3.72348566 -0.46442898\n",
      "  4.74510775 -0.61075169  0.87224032 -2.2857234  -2.41324808 -0.15238106\n",
      "  1.60649825  0.08408569  1.74978697 -0.24617482 -0.76487867  0.30684785\n",
      " -0.91550352  0.51880044  0.08042436  4.06950204  0.20583098  0.34820269\n",
      "  1.06530656 -0.80905019  1.38998916  0.03506348  1.53665194 -1.18913595\n",
      "  1.01819343  2.08602207  0.39963351  2.71747479  0.75640664 -1.96206752\n",
      " -1.69741412  1.21446267 -1.52403124  3.33677076 -0.34834044  1.83811599\n",
      "  2.13385246 -1.35241333  2.27921666 -0.88507007 -0.78177641  0.3795037\n",
      "  0.61971256  1.24897169 -0.60242009  0.93324332  0.99830626 -1.29119163\n",
      "  2.03054509 -0.37120962 -0.43012404  1.86261398  0.35522562  1.06676602\n",
      "  1.88909913 -0.45594802 -0.59492462  0.19161234 -0.5014968   0.04108744\n",
      " -2.52053754 -0.4543712   1.54846857 -1.52132405]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#  素性追加  ##\n",
    "f = open('./text_body_part1.csv', 'r') \n",
    "#f = open('./fit_data/text_body_part.csv', 'r') \n",
    "dataReader = csv.reader(f)\n",
    "for line , row in zip(vectors , dataReader):\n",
    "    np.append(line, float(row[0]))\n",
    "    # line.append(float(row[0]))\n",
    "    # ↑word2vecの場合は，`vectors`がnumpyベクトルであるため，別のメソッドを使っている\n",
    "\n",
    "f1 = open('./text_sick1.csv', 'r')\n",
    "#f1 = open('./fit_data/text_sick.csv', 'r')\n",
    "dataReader1 = csv.reader(f1)\n",
    "for line , row in zip(vectors , dataReader1):\n",
    "    np.append(line, float(row[0]))\n",
    "    # line.append(float(row[0]))\n",
    "\n",
    "f2 = open('./fit_data/text_word_count.csv', 'r')\n",
    "dataReader2 = csv.reader(f2)\n",
    "for line, row in zip(vectors , dataReader2):\n",
    "    np.append(line, float(row[0]))\n",
    "    # line.append(float(row[0]))\n",
    "\n",
    "\n",
    "#  pmi  #\n",
    "ar_1 = vectors\n",
    "eps = 1e-8\n",
    "cm = np.array(ar_1)\n",
    "pmi = np.zeros_like(cm,dtype=np.float32)\n",
    "nsum = np.sum(cm)\n",
    "ssum = np.sum(cm,axis=0)\n",
    "for i in range(cm.shape[0]):\n",
    "    if i==200:\n",
    "         break\n",
    "    for j in range(cm.shape[1]):\n",
    "         pw = np.log2(cm[i,j] * nsum / (ssum[j] * ssum[i]) + eps)         \n",
    "         pmi[i,j] = max(0,pw)\n",
    "vectors = pmi\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "f = open('./tsutsuzi_label_sample2.csv', 'r') \n",
    "dataReader = csv.reader(f)\n",
    "for line , row in zip(wakatis_bow , dataReader):\n",
    "    if str(row[0]) != \"0.0\":\n",
    "        line.append(str(row[0]))\n",
    "  \n",
    "f1 = open('./tsutsuzi_label_sample2.csv', 'r')\n",
    "dataReader1 = csv.reader(f1)\n",
    "for line1 , row1 in zip(wakatis_bow , dataReader1):\n",
    "    if str(row1[1]) != \"0.0\":\n",
    "        line1.append(str(row1[1]))\n",
    " \n",
    "f2 = open('./tsutsuzi_label_sample2.csv', 'r')\n",
    "dataReader2 = csv.reader(f2)\n",
    "for line2 , row2 in zip(wakatis_bow , dataReader2):\n",
    "    if str(row2[2]) != \"0.0\":\n",
    "        line2.append(str(row2[2]))\n",
    "\n",
    "f3 = open('./tsutsuzi_label_sample2.csv', 'r')\n",
    "dataReader3 = csv.reader(f3)\n",
    "for line3 , row3 in zip(wakatis_bow , dataReader3):\n",
    "    if str(row3[3]) != \"0.0\":\n",
    "        line3.append(str(row3[3]))\n",
    "\n",
    "f4 = open('./tsutsuzi_label_sample2.csv', 'r')\n",
    "dataReader4 = csv.reader(f4)\n",
    "for line4 , row4 in zip(wakatis_bow , dataReader4):\n",
    "    if str(row4[4]) != \"0.0\":\n",
    "        line4.append(str(row4[4]))\n",
    " \n",
    "f5 = open('./tsutsuzi_label_sample2.csv', 'r')\n",
    "dataReader5 = csv.reader(f5)\n",
    "for line5 , row5 in zip(wakatis_bow , dataReader5):\n",
    "    if str(row5[5]) != \"0.0\":\n",
    "        line5.append(str(row5[5]))\n",
    "\n",
    "f6 = open('./tsutsuzi_label_sample2.csv', 'r')\n",
    "dataReader6 = csv.reader(f6)\n",
    "for line6 , row6 in zip(wakatis_bow , dataReader6):\n",
    "    if str(row6[6]) != \"0.0\":\n",
    "        line6.append(str(row6[6]))\n",
    "  \n",
    "f7 = open('./tsutsuzi_label_sample2.csv', 'r')\n",
    "dataReader7 = csv.reader(f7)\n",
    "for line7 , row7 in zip(wakatis_bow , dataReader7):\n",
    "    if str(row7[7]) != \"0.0\":\n",
    "        line7.append(str(row7[7]))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(wakatis_bow)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.9)\n",
    "print(dictionary)\n",
    "vectors_bow = np.array([vec2dense(dictionary.doc2bow(wakatis_bow[i]), len(dictionary)) for i in range(len(wakatis_bow))], dtype='f8')\n",
    "print(vectors_bow[1])\n",
    "\n",
    "##  LDA  ##\n",
    "combined_vectors = []\n",
    "for bow, w2v, in zip(wakatis_bow1, vectors):\n",
    "     combined_vectors.append(np.hstack((bow,w2v)))\n",
    "print(len(combined_vectors[0]))\n",
    "print(len(combined_vectors))\n",
    "\n",
    "##  つつじ  ##\n",
    "np.insert(combined_vectors,[200],vectors_bow,axis=1)\n",
    "combined_vectors1 = []\n",
    "for bow, w2v in zip(vectors_bow, combined_vectors):\n",
    "    combined_vectors1.append(np.hstack((bow,w2v)))\n",
    "print(len(combined_vectors1[0]))\n",
    "print(len(combined_vectors1))\n",
    "\n",
    "print(combined_vectors[0])\n",
    "print(combined_vectors1[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = np.loadtxt('lastdata1_ans.csv',  delimiter=',', dtype='int')\n",
    "labels = np.loadtxt('./sample_LDA_setdata/all_smptons_set1000_ans.csv',  delimiter=',', dtype='int')\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "scores_dict_format: Dict[str, List[float]] = {\n",
    "    'precision': [], 'recall': [], 'f_measure': [], 'precision_weighted':[], 'recall_weighted':[], 'f1_weighted':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/s15t283/svm_test_d/env/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "1it [00:04,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.93      0.89       113\n",
      "           1       0.83      0.67      0.74        58\n",
      "\n",
      "    accuracy                           0.84       171\n",
      "   macro avg       0.84      0.80      0.81       171\n",
      "weighted avg       0.84      0.84      0.84       171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s15t283/svm_test_d/env/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\r",
      "2it [00:09,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87       116\n",
      "           1       0.75      0.67      0.71        54\n",
      "\n",
      "    accuracy                           0.82       170\n",
      "   macro avg       0.80      0.78      0.79       170\n",
      "weighted avg       0.82      0.82      0.82       170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s15t283/svm_test_d/env/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\r",
      "3it [00:13,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90       123\n",
      "           1       0.77      0.64      0.70        47\n",
      "\n",
      "    accuracy                           0.85       170\n",
      "   macro avg       0.82      0.78      0.80       170\n",
      "weighted avg       0.84      0.85      0.84       170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s15t283/svm_test_d/env/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\r",
      "4it [00:18,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.91       117\n",
      "           1       0.80      0.77      0.79        53\n",
      "\n",
      "    accuracy                           0.87       170\n",
      "   macro avg       0.85      0.84      0.85       170\n",
      "weighted avg       0.87      0.87      0.87       170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s15t283/svm_test_d/env/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\r",
      "5it [00:22,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.85      0.87       119\n",
      "           1       0.68      0.76      0.72        51\n",
      "\n",
      "    accuracy                           0.82       170\n",
      "   macro avg       0.79      0.81      0.80       170\n",
      "weighted avg       0.83      0.82      0.83       170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s15t283/svm_test_d/env/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\r",
      "6it [00:27,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.87       112\n",
      "           1       0.76      0.67      0.72        58\n",
      "\n",
      "    accuracy                           0.82       170\n",
      "   macro avg       0.80      0.78      0.79       170\n",
      "weighted avg       0.81      0.82      0.81       170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s15t283/svm_test_d/env/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\r",
      "7it [00:32,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.87       125\n",
      "           1       0.65      0.62      0.64        45\n",
      "\n",
      "    accuracy                           0.81       170\n",
      "   macro avg       0.76      0.75      0.75       170\n",
      "weighted avg       0.81      0.81      0.81       170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s15t283/svm_test_d/env/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\r",
      "8it [00:36,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88       111\n",
      "           1       0.77      0.75      0.76        59\n",
      "\n",
      "    accuracy                           0.84       170\n",
      "   macro avg       0.82      0.81      0.82       170\n",
      "weighted avg       0.83      0.84      0.83       170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s15t283/svm_test_d/env/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\r",
      "9it [00:41,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86       111\n",
      "           1       0.74      0.71      0.72        59\n",
      "\n",
      "    accuracy                           0.81       170\n",
      "   macro avg       0.79      0.79      0.79       170\n",
      "weighted avg       0.81      0.81      0.81       170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s15t283/svm_test_d/env/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "10it [00:45,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.88       116\n",
      "           1       0.72      0.80      0.75        54\n",
      "\n",
      "    accuracy                           0.84       170\n",
      "   macro avg       0.81      0.82      0.82       170\n",
      "weighted avg       0.84      0.84      0.84       170\n",
      "\n",
      "\n",
      "class: -1(macro_mean) \n",
      "precision: 0.8685719217204758 \n",
      "precision_weighted: 0.8685719217204758 recall 0.8889907109048816 recall_weighted: 0.8685719217204758 f_measure 0.8782198431313836 f1_weighted: 0.8685719217204758\n",
      "\n",
      "class:  1(macro_mean) \n",
      "precision: 0.7478457367758407 \n",
      "precision_weighted: 0.7478457367758407 recall 0.7064228550389894 recall_weighted: 0.7478457367758407 f_measure 0.7246202227327534 f1_weighted: 0.7478457367758407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results: Dict[int, Dict[str, List[float]]\n",
    "              ] = {-1: deepcopy(scores_dict_format), 1: deepcopy(scores_dict_format)}\n",
    "accuracies = []\n",
    "for train_index, test_index in tqdm(kf.split(combined_vectors, labels)):\n",
    "    train_data = [combined_vectors[idx] for idx in train_index]\n",
    "    #train_data = np.array([combined_vectors[idx] for idx in train_index])\n",
    "    train_labels = [labels[idx] for idx in train_index]\n",
    "    #train_labels = np.array([labels[idx] for idx in train_index])\n",
    "    test_data = [combined_vectors[idx] for idx in test_index]\n",
    "    test_labels = [labels[idx] for idx in test_index]\n",
    "                   \n",
    "    #model = als.FMClassification(n_iter=1000, init_stdev=0.1, rank=2, l2_reg_w=0.1, l2_reg_V=0.5)\n",
    "    #model.fit(csr_matrix(train_data), train_labels)\n",
    "    \n",
    "    ##  ロジスティックス回帰  ##\n",
    "    #model = linear_model.LogisticRegression(random_state=0)\n",
    "    #model.fit(train_data, train_labels)\n",
    "    \n",
    "    ##  SVM  ##\n",
    "    #model = SVC(kernel='rbf', random_state=0, C=1000)#, gamma=\"scale\")\n",
    "    #model.fit(train_data, train_labels)\n",
    "    \n",
    "    ## Multi layer perceptron ##\n",
    "    model = MLPClassifier(solver='sgd', batch_size=128)\n",
    "    model.fit(train_data, train_labels)\n",
    "    \n",
    "    pred_labels = model.predict(test_data)\n",
    "    accuracies.append(accuracy_score(test_labels, pred_labels))\n",
    "    p, r, f, _ = precision_recall_fscore_support(test_labels, pred_labels)\n",
    "    results[-1]['precision'].append(p[0])\n",
    "    results[-1]['recall'].append(r[0])\n",
    "    results[-1]['f_measure'].append(f[0])\n",
    "    results[1]['precision'].append(p[1])\n",
    "    results[1]['recall'].append(r[1])\n",
    "    results[1]['f_measure'].append(f[1])\n",
    "    \n",
    "    results[-1]['precision_weighted'].append(p[0])\n",
    "    results[1]['precision_weighted'].append(p[1])\n",
    "    results[-1]['recall_weighted'].append(p[0])\n",
    "    results[1]['recall_weighted'].append(p[1])\n",
    "    results[-1]['f1_weighted'].append(p[0])\n",
    "    results[1]['f1_weighted'].append(p[1])\n",
    "    \n",
    "    print(classification_report(test_labels,model.predict(test_data)))\n",
    "\n",
    "#    for c , p , t in zip(test_labels , pred_labels , lines):\n",
    "#        if c != p:\n",
    "#            print(c,p,t)\n",
    "\n",
    "    #df = list(d.items())\n",
    "    #print (d.keys())\n",
    "    \n",
    "    # print('class: -1', results[-1])\n",
    "print('\\nclass: -1(macro_mean)',\n",
    "      '\\nprecision:', scores2mean(results[-1]['precision']),\n",
    "      '\\nprecision_weighted:', scores2mean(results[-1]['precision_weighted']),\n",
    "      'recall', scores2mean(results[-1]['recall']),\n",
    "      'recall_weighted:', scores2mean(results[-1]['recall_weighted']),  \n",
    "      'f_measure', scores2mean(results[-1]['f_measure']),\n",
    "      'f1_weighted:', scores2mean(results[-1]['f1_weighted']))\n",
    "# print('class:  1', results[1])\n",
    "print('\\nclass:  1(macro_mean)',\n",
    "      '\\nprecision:', scores2mean(results[1]['precision']),\n",
    "      '\\nprecision_weighted:', scores2mean(results[1]['precision_weighted']),\n",
    "      'recall', scores2mean(results[1]['recall']),\n",
    "      'recall_weighted:', scores2mean(results[1]['recall_weighted']),\n",
    "      'f_measure', scores2mean(results[1]['f_measure']),\n",
    "      'f1_weighted:', scores2mean(results[1]['f1_weighted']))\n",
    "\n",
    "#print(((scores2mean(results[-1]['precision_weighted']))+(scores2mean(results[1]['precision_weighted']))) / 2.0)\n",
    "#print(((scores2mean(results[-1]['recall_weighted']))+(scores2mean(results[1]['recall_weighted']))) / 2.0)\n",
    "#print(((scores2mean(results[-1]['f1_weighted']))+(scores2mean(results[1]['f1_weighted']))) / 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svm \n",
    "#### word2vec\n",
    "Fclass: -1(macro_mean) \n",
    "precision: 0.7559140299372962 recall 0.7144076721450139 f_measure 0.7323472242955001\n",
    "\n",
    "class:  1(macro_mean) \n",
    "precision: 0.7122171063423284 recall 0.7582533810218722 f_measure 0.7324821540127525\n",
    "\n",
    "#### word2vec+病名\n",
    "class: -1(macro_mean) \n",
    "precision: 0.7558769307820346 recall 0.7144076721450139 f_measure 0.7323006004212901\n",
    "\n",
    "class:  1(macro_mean) \n",
    "precision: 0.7123384347274013 recall 0.7586004614628685 f_measure 0.7326731272440975\n",
    "\n",
    "#### word2vec+部位\n",
    "class: -1(macro_mean) \n",
    "precision: 0.7510065186004868 recall 0.7155715164706229 f_measure 0.7305426192252337\n",
    "\n",
    "class:  1(macro_mean) \n",
    "precision: 0.7113255195828777 recall 0.7505846411662427 f_measure 0.728373567598106\n",
    "\n",
    "#### word2vec+文長 \n",
    "class: -1(macro_mean) \n",
    "precision: 0.7549770930003593 recall 0.7144076721450139 f_measure 0.7318203752580128\n",
    "\n",
    "class:  1(macro_mean) \n",
    "precision: 0.7119840061092282 recall 0.7570905903241977 f_measure 0.7317339870218801\n",
    "\n",
    "#### word2vec+3素性\n",
    "class: -1(macro_mean) \n",
    "precision: 0.7497172677967044 recall 0.7143950458823876 f_measure 0.7291640620550615\n",
    "\n",
    "class:  1(macro_mean) \n",
    "precision: 0.7103821947808755 recall 0.7498081261016211 f_measure 0.7273705901392942\n",
    "\n",
    "### MLP\n",
    "#### word2vec\n",
    "class: -1(macro_mean) \n",
    "precision: 0.7190290388941041 recall 0.7316442283000363 f_measure 0.7233889547336847\n",
    "\n",
    "class:  1(macro_mean) \n",
    "precision: 0.7057989254463405 recall 0.6968113618494095 f_measure 0.6993334167722509\n",
    "\n",
    "#### word2vec+病名\n",
    "class: -1(macro_mean) \n",
    "precision: 0.7265466291598826 recall 0.7153768202716453 f_measure 0.7187895540245088\n",
    "\n",
    "class:  1(macro_mean) \n",
    "precision: 0.7006216137788077 recall 0.7133205601221 f_measure 0.7046645183156733\n",
    "\n",
    "#### word2vec+部位\n",
    "class: -1(macro_mean) \n",
    "precision: 0.7261581534651302 recall 0.7130075463545308 f_measure 0.7171599673538495\n",
    "\n",
    "class:  1(macro_mean) \n",
    "precision: 0.6993122187295194 recall 0.7165268780924017 f_measure 0.7051272209948094\n",
    "\n",
    "#### word2vec+文長\n",
    "class: -1(macro_mean) \n",
    "precision: 0.7390313823692203 recall 0.7199383524765134 f_measure 0.7271926104241068\n",
    "\n",
    "class:  1(macro_mean) \n",
    "precision: 0.7080127913895308 recall 0.7282764160763009 f_measure 0.715977786588644\n",
    "\n",
    "#### word2vec+3素性\n",
    "class: -1(macro_mean) \n",
    "precision: 0.7232418720450512 recall 0.7210102333547977 f_measure 0.7199297460710744\n",
    "\n",
    "class:  1(macro_mean) \n",
    "precision: 0.703361281312904 recall 0.7072590589736242 f_measure 0.7031391663512795\n",
    "\n",
    "### ロジスティックス回帰\n",
    "#### word2vec\n",
    "\n",
    "\n",
    "#### word2vec+病名\n",
    "\n",
    "\n",
    "#### word2vec+部位\n",
    "\n",
    "\n",
    "#### word2vec+文長\n",
    "\n",
    "\n",
    "#### word2vec+3素性\n",
    "class: -1(macro_mean) \n",
    "precision: 0.7226595176162555 recall 0.6885464518934363 f_measure 0.7031535607992214\n",
    "\n",
    "class:  1(macro_mean) \n",
    "precision: 0.6843165868658938 recall 0.7213928781468446 f_measure 0.7000704210006579"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-9-42fc1ebba3b5>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-42fc1ebba3b5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    1 -1 それはどうかな庶民は前半は無難な質問を投げかけつつ痺れを切らしたインサイダーの誘導が始まったあたりで全力で答えを考えてなんとなく近い言葉ばかり発して行くとインサイダーが乗り出すからそこを狙ってインサイダーを誘導するのが頭いい。\u001b[0m\n\u001b[0m                                                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "1 -1 それはどうかな庶民は前半は無難な質問を投げかけつつ痺れを切らしたインサイダーの誘導が始まったあたりで全力で答えを考えてなんとなく近い言葉ばかり発して行くとインサイダーが乗り出すからそこを狙ってインサイダーを誘導するのが頭いい。\n",
    "\n",
    "1 -1 もぐもぐ症候群とは定期的にグウェンを摂取しないと震えや動悸、めまい、吐き気、手足の痺れなどの症状が出る病気。\n",
    "\n",
    "-1 1 発熱で休んだ元気な幼児がいます(しんどい\n",
    "\n",
    "-1 1 痛みは数日でおさまりましたが、下顎の痺れは半年経った現在でも少し残ってしまいました。\n",
    "\n",
    "-1 1 インフルエンザとかになって有休消化したいとか思ってたけど間違ってたしんどいわ。\n",
    "\n",
    "1 -1 ）」ってなったんだけど帰って鏡見たら目がすげー充血してるし目の下に漫画みたいな濃さのクマもできてたのでびっくらこいた（でも元気）\n",
    "\n",
    "1 -1 めまいが…\n",
    "\n",
    "1 -1 思えば、先週の木曜日から次男発熱。\n",
    "\n",
    "1 -1 っていう声とニヤニヤと動悸が止まらないんですけどどうしたらいいですか？\n",
    "\n",
    "1 -1 自分キモすぎて吐き気するわwなんでこんな性格なんだろね\n",
    "\n",
    "1 -1 カンタくんに影響されて始めたランニング🏃♂️🏃♀️週4〜5回走るのを2週間続けたら、息切れからの回復速度が速くなった！\n",
    "\n",
    "-1 1 夏のめまいfeat.\n",
    "\n",
    "-1 1 頭痛薬キメてたせいで献血できなかった………元気です！\n",
    "\n",
    "-1 1 昨日から少し頭痛していて、薬飲んでもあんま効かねーし、更に今朝起きたら動くたびに痛みがずきずきくるもんだからなんぞ？\n",
    "\n",
    "1 -1 雨からの急な晴れの日は動悸がしてホントしんどい…心を無にして電車に乗る夏がもうすぐやってくる！\n",
    "\n",
    "1 -1 晴れてるのに関節痛がひどい.\n",
    "\n",
    "1 -1 手の甲には食器用洗剤に負けて発疹が出来ちょったけど、入院してからどっちも綺麗に治った。\n",
    "\n",
    "-1 1 動悸というか、急に心臓がバクバクするのは不整脈？\n",
    "\n",
    "-1 1 うわ～立ったら、少しめまいっぽくなった…そして、飛蚊症も、ちらっとした(-\"\"-;)最近、ゲームのし過ぎかな…眼精疲労だなぁ\n",
    "\n",
    "-1 1 これっていう事なんだとして関節痛が過ぎている気持ちがするんだけど \n",
    "\n",
    "-1 1 頭痛いい眠いしだるいし足筋肉痛っぽいし\n",
    "\n",
    "1 -1 その後に突発性発疹が出るかもねとも言われた。\n",
    "\n",
    "-1 1 っと思ってたけど今朝すっかり熱は下がりました。\n",
    "\n",
    "-1 1 発疹は気になるけど、発熱もなく、ミルクもしっかり飲めてるなら様子見でいいんじゃないか」と、安心していいんだか、じゃあこの発疹はなんなんだと心配は消えず。\n",
    "\n",
    "1 -1 7日から5連勤の予定だったけど5日間発熱し倒してて土曜日は39度、日曜日は吐き気酷くて休日診療所で胃腸炎と言われ1日分の薬を貰い、どーにか10日に仕事復帰したのにその夜に食べた何かしらで全身蕁麻疹でて、次の日の朝に赤い発疹に変わり、休憩時間に行った病院で\n",
    "\n",
    "1 -1 吐き気やば\n",
    "\n",
    "1 -1 )皆もこんな動悸するんだ？\n",
    "\n",
    "-1 1 まぁ一週間ほどこのまま使ってみますかー時々触ってどれくらい熱くなるのか見とこもともと結構発熱するけど\n",
    "\n",
    "-1 1 流石に39度近くまで行った発熱は一晩じゃ治らんわな。\n",
    "\n",
    "-1 1 昨日紹介状もらって病院行ったけど手足口病じゃなかった笑2度目の突発性発疹！\n",
    "\n",
    "1 -1 ●娘の症状足の裏から、ふともも下にかけて赤い発疹がたくさん出る遅れて、手指、腕にかけて発疹翌日手足の指先付近に水疱がでる\n",
    "\n",
    "-1 1 風邪キメたんですけど発熱と咳に効く総合風邪薬買った直後本当にやばいのは鼻の方だって事に気づいてしまったので鼻に効くやつ買えばよかったわ〜〜〜って思いました \n",
    "\n",
    "1 -1 そう言えば昨日は卵焼きとビールしか胃に入れてないな、朝起きた時吐き気やばくてびっくりした\n",
    "\n",
    "1 -1 うえー頭痛\n",
    "\n",
    "-1 1 まじで関節痛やばい仮病で仕事休んで平日休みたい\n",
    "\n",
    "-1 1 昨日まで40度近くあった熱も今日は平熱で安心！\n",
    "\n",
    "1 -1 知らない人がぶつかってくるだけで吐き気する\n",
    "\n",
    "-1 1 自分の画力が低すぎて地の底を這ってるレベルなせいで動悸してきた\n",
    "\n",
    "-1 1 発熱 \n",
    "\n",
    "-1 1 ただし頭の後ろにゴムを回すタイプだと髪を巻き込むのが気になるし、長さをうまく調整しないと締め付け感で軽い頭痛っぽくなる。\n",
    "\n",
    "-1 1 昔からそうだけど、自分の予定を急に誰かに乱されるのが実際に吐き気を催すくらいに嫌いっぽいなぁどうにかならんかなコレ\n",
    "\n",
    "1 -1 朝起きたら雪山でゴーグルしなかった時みたいに目が疲れて充血してるんですけど昨日の羽生結弦さんがあまりにも眩しすぎたからかな…👼🏻🌤\n",
    "\n",
    "-1 1 腰は3回目だけど、入院中とか全く痛くなくて、今回痺れまくってるの何これ…\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class: -1(macro_mean) \n",
    "precision: 0.8597758500776866 \n",
    "precision_weighted: 0.8597758500776866 recall 0.8883631038211288 recall_weighted: 0.8597758500776866 f_measure 0.8734145898578933 f1_weighted: 0.8597758500776866\n",
    "\n",
    "class:  1(macro_mean) \n",
    "precision: 0.7339829286358177 \n",
    "precision_weighted: 0.7339829286358177 recall 0.6740374147239484 recall_weighted: 0.7339829286358177 f_measure 0.6994706189692754 f1_weighted: 0.7339829286358177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
